#应用名
spring.application.name=data-server

#端口号
server.port=9010

#本机地址和注册地址
eureka.instance.hostname=192.100.1.231
eureka.client.service-url.defaultZone=http://192.100.1.111:8761/eureka/, http://192.100.1.112:8762/eureka/

#消息队列的配置
spring.cloud.stream.instance-count=3
spring.cloud.stream.instance-index=0
spring.cloud.stream.kafka.binder.brokers=192.100.1.112:9092
spring.cloud.stream.kafka.binder.zk-nodes=192.100.1.112:2181
spring.cloud.stream.kafka.binder.min-partition-count=1
spring.cloud.stream.bindings.input.destination=Log
#消费组不同，对消息的消费情况也不同；如果把消费组改变了与之前不同的名字，那么将会消费之前发到kafka队列中的所有消息
#所以这里我有一个问题就是：是不是发到消息队列的消息会持久化到磁盘上，即使被一个消费组消费了也不会销毁？
spring.cloud.stream.bindings.input.group=data-server
#设置消费失败后最大重试次数，默认是3
spring.cloud.stream.bindings.input.consumer.max-attempts=1
spring.cloud.stream.bindings.input.consumer.concurrency=1
spring.cloud.stream.bindings.input.consumer.partitioned=false
spring.cloud.stream.bindings.output.destination=output


#MySQL数据库的配置
spring.datasource.url=jdbc:mysql://192.100.1.111:3306/dljyxt?serverTimezone=GMT
spring.datasource.username=root
spring.datasource.password=123456
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource

#ElastricSearch的配置
#spring.data.elasticsearch.cluster-nodes=192.100.1.60:9300, 192.100.1.16:9300, 192.100.1.17:9300
#spring.data.elasticsearch.cluster-name=my-ela
#spring.data.elasticsearch.repositories.enabled=false
